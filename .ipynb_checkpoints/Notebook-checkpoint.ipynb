{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann Machine Learning with Mean-Field Methods\n",
    "\n",
    "A tutorial for VDSP-ESI Winter School 2020: Machine Learning in Physics. <br/>\n",
    "\n",
    "by Marylou Gabri√© (NYU - Flatiron Institute) & Alia Abbara (ENS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of content\n",
    "\n",
    "1. [The RBM Object](#class)\n",
    "2. [Monte Carlo Markov Chain Training](#mcmc)\n",
    "3. [Naive Mean-field Training](#nmf) <br/> \n",
    "    3.1 Mean-field approximation of the log-likelihood<br/> \n",
    "    3.2 Training implementation \n",
    "4. [Thouless-Anderson-Palmer Training](#tap)\n",
    "3. [Comparison](#comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter notebook start-up\n",
    "\n",
    "To start off the tutorial, a lot of functions are readily provided. You should observe and understand these functions. \n",
    "They will be a good starting point for the cases you will have to code up yourself.\n",
    "\n",
    "We start by \n",
    "- charging useful module and options\n",
    "- load the MNIST dataset for experiments\n",
    "- define useful functions for the rest of the tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.rc('image', cmap='gray')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# prepare binary MNIST training images\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype(np.float32) / 255\n",
    "x_train[x_train > 0] = 1\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], 784))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def prepare_batches(x_train, batch_size):\n",
    "    batches = x_train.shape[0]//batch_size\n",
    "    batch_list = []\n",
    "    for batch in range(batches):\n",
    "        x_batch = x_train[batch*batch_size: (batch+1)*batch_size, :]\n",
    "        batch_list.append(x_batch)\n",
    "    return batch_list\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=class><a/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The RBM object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an python class to store and manipulate models of RBMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM:\n",
    "    def __init__(self, N, M, eta=1e-4, gamma=1e-3, batch_size=100):\n",
    "        self.N = N  # number of visible units\n",
    "        self.M = M  # number of hidden units\n",
    "        self.W = np.random.randn(N, M) / np.sqrt(N)\n",
    "        self.a = np.random.randn(N)\n",
    "        self.b = np.random.randn(M)\n",
    "        \n",
    "        self.eta = eta\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        ##self.history = []\n",
    "\n",
    "    def sample_h_given_x(self, x):\n",
    "        '''\n",
    "        input : input batch (P x N)\n",
    "        output : hidden batch (P x M)\n",
    "        '''\n",
    "        prob = sigmoid(x.dot(self.W) + self.b)\n",
    "        bool_test = np.random.rand(x.shape[0], self.M) < prob\n",
    "        return bool_test.astype(float)\n",
    "\n",
    "    def sample_x_given_h(self, h):\n",
    "        '''\n",
    "        input : hidden batch (P x M)\n",
    "        output : input batch (P x N)\n",
    "        '''\n",
    "        prob = sigmoid(h.dot(self.W.T) + self.a)\n",
    "        bool_test = np.random.rand(h.shape[0], self.N)  < prob\n",
    "        return bool_test.astype(float)\n",
    "\n",
    "    def run_monte_carlo(self, x_init, steps=3):\n",
    "        '''\n",
    "        input : input batch (P x N)\n",
    "        output : hidden batch (P x N), (P x M)\n",
    "        '''\n",
    "        x = x_init\n",
    "        for step in range(steps):\n",
    "            h = self.sample_h_given_x(x)\n",
    "            x = self.sample_x_given_h(h)\n",
    "        return x, h\n",
    "                \n",
    "    def plot_mcmc_chains(self, x):\n",
    "        size = x.shape[0] if x.shape[0] < 10 else 10\n",
    "        idx = np.random.randint(x.shape[0], size=size)\n",
    "        x_batch = x_train[idx, :]\n",
    "        x_eq, h_eq = self.run_monte_carlo(x_batch)\n",
    "        x_eq = x_eq.reshape(10, 28, 28)\n",
    "        plt.figure(figsize=(size,1))\n",
    "        for i,im in enumerate(x_eq):\n",
    "            ax=plt.subplot(1,size,i+1)\n",
    "            ax.imshow(im)\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "    def save(self, filename):\n",
    "        with open(filename + '.pickle', 'wb') as file:\n",
    "            pickle.dump(self, file)\n",
    "    \n",
    "\n",
    "def load(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "         rbm = pickle.load(file)\n",
    "    return rbm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=mcmc></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Monte Carlo Markov Chain Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we give an example of implementation of the contrastive-divergence training. \n",
    "- A first function computes the updates of the gradients for a given mini-batch of data. \n",
    "- A second function uses the previous one and loops over the mini-batches.\n",
    "\n",
    "Some lines are commented with two ## and should remain so for the begining. We will define later the functions they require and re-run them at this point.\n",
    "\n",
    "In the following cell we launch the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_updates_mcmc(self, x_batch):\n",
    "    h1 = self.sample_h_given_x(x_batch)\n",
    "    x, h = self.run_monte_carlo(x_batch)\n",
    "\n",
    "    DW = np.einsum('ki, ka -> ia', x_batch, h1)\n",
    "    DW -= np.einsum('ki, ka -> ia', x, h)\n",
    "    self.W = self.W + self.eta * DW\n",
    "\n",
    "    Da = np.sum(x_batch - x, axis=0)\n",
    "    self.a = self.a + self.eta * Da\n",
    "\n",
    "    Db = np.sum(h1 - h, axis=0)\n",
    "    self.b = self.b + self.eta * Db\n",
    "    \n",
    "        \n",
    "def fit(self, x_train, epochs=10):\n",
    "    batch_list = prepare_batches(x_train, \n",
    "                                self.batch_size)\n",
    "    \n",
    "    ##ell_init = compute_mean_field_likelihood(self, batch_list[0])\n",
    "    ##self.history =[ell_init]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch {:d}\".format(epoch))\n",
    "        for x_batch in batch_list:\n",
    "            gradient_updates_mcmc(self, x_batch)\n",
    "            ##ell = compute_mean_field_likelihood(self, x_batch)\n",
    "        ##self.history.append(ell)\n",
    "        ##print('MF loglikelihood', ell) \n",
    "        if epoch % 5 == 0:\n",
    "            self.plot_mcmc_chains(x_batch)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm = RBM(784,100)\n",
    "fit(rbm, x_train[:6000,:], epochs=5)\n",
    "rbm.save('rbm_mcmc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=nmf></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Naive Mean-field Training\n",
    "\n",
    "## 3.1 Mean-field approximation of the log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Section C of the tutorial text and answer corresponding questions!\n",
    "\n",
    "- Write a function to compute the mean-field likelihood using the 'run_mean_field' function to compute the fixed points of the magnetizations. Similarly to the constrative divergence scheme, we will only run the iteration for a few steps instead of waiting for convergence.\n",
    "\n",
    "- Once this is done, you can uncomment what is necessary in the MCMC training above to store the mean-field log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mean_field(self, x_init, steps=3):\n",
    "    '''\n",
    "    input : input batch (P x N)\n",
    "    output : input and hidden mean-field means (P x N), (P x M)\n",
    "    '''\n",
    "    mx = x_init\n",
    "    for step in range(steps):\n",
    "        mh = sigmoid(mx.dot(self.W) + self.b)\n",
    "        mx = sigmoid(mh.dot(self.W.T) + self.a)\n",
    "    return mx, mh\n",
    "\n",
    "def compute_mean_field_likelihood(self, x_batch):\n",
    "    # First find the mean-field fixed \n",
    "    # points in the neighborhood of the batch\n",
    "\n",
    "    return ell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Training implementation\n",
    "\n",
    "Now it is your turn to implement the mean-field learning, and launch it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_updates_mean_field(self, x_batch):\n",
    "\n",
    "    \n",
    "    \n",
    "def fit_mean_field(self, x_train, epochs=10):\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm = RBM(784,100)\n",
    "fit_mean_field(rbm, x_train[:6000,:], epochs=50)\n",
    "rbm.save('rbm_nmf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=nmf></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Thouless-Anderson-Palmer Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we propose an implementation for the training based on the TAP approximation. \n",
    "- Check that you agree.\n",
    "- What is different from your naive mean-field code.\n",
    "- Launch a train to be able to compare this new learning algorithms with the previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tap(self, x_init, steps=3):\n",
    "    '''\n",
    "    input : input batch (P x N)\n",
    "    output : hidden batch (P x M)\n",
    "    '''\n",
    "    mx = x_init\n",
    "    for step in range(steps):\n",
    "        onsager = (mx - mx ** 2).dot(self.W**2) * (1/2 - mh)\n",
    "        mh = sigmoid(mx.dot(self.W) + self.b + onsager)\n",
    "        onsager =(mh - mh ** 2).dot(self.W**2) * (1/2 - mx)\n",
    "        mx = sigmoid(mh.dot(self.W.T) + self.a + onsager)\n",
    "    return mx, mh\n",
    "\n",
    "def gradient_updates_tap(self, x_batch):\n",
    "    mh1 = sigmoid(x_batch.dot(self.W) + self.b)\n",
    "    mx, mh = run_mean_field(self, x_batch)\n",
    "\n",
    "    # not rescaled by the size of the batch\n",
    "    DW = np.einsum('ki, ka -> ia', x_batch, mh1)\n",
    "    DW -= np.einsum('ki, ka -> ia', mx, mh)\n",
    "    DW += np.einsum('ia, ki, ka -> ia',self.W, mx - mx**2, mh - mh**2)\n",
    "    self.W = self.W + self.eta * DW - self.eta * self.gamma * self.W\n",
    "\n",
    "    Da = np.sum(x_batch - mx, axis=0)\n",
    "    self.a = self.a + self.eta * Da\n",
    "\n",
    "    Db = np.sum(mh1 - mh, axis=0)\n",
    "    self.b = self.b + self.eta * Db\n",
    "\n",
    "    \n",
    "def fit_tap(self, x_train, epochs=10):\n",
    "    batch_list = prepare_batches(x_train, \n",
    "                                self.batch_size)\n",
    "    \n",
    "    ell_init = compute_mean_field_likelihood(self, batch_list[0])\n",
    "    self.history = [ell_init]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch {:d}\".format(epoch))\n",
    "        \n",
    "        for x_batch in batch_list:\n",
    "            gradient_updates_tap(self, x_batch)\n",
    "            \n",
    "        ell = compute_mean_field_likelihood(self, x_batch)\n",
    "        self.history.append(ell)\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            self.plot_mcmc_chains(x_batch)\n",
    "            print('MF loglikelihood', ell)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm = RBM(784,100)\n",
    "fit_tap(rbm, x_train[:6000,:], epochs=50)\n",
    "rbm.save('rbm_tap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=comp><a/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have stored the mean-field approxiamtion of the likelihood we can retrieve this information and compare the different algorithms.\n",
    "- Use the 'load' function defined in [Part 1](#class) to get all the mean-field likelihoods logs from the different types of training.\n",
    "- Use the matplotlib python package (imported under the alias 'plt') to visualize the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbasecondaf780b1061bbf45e98b9e6345ba0ba200"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
